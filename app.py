"""
Rule-Based Chatbot vá»›i Elasticsearch

Äá»ƒ sá»­ dá»¥ng Elasticsearch:
1. Táº£i vÃ  cÃ i Ä‘áº·t Elasticsearch tá»«: https://www.elastic.co/downloads/elasticsearch
2. Cháº¡y Elasticsearch trÃªn localhost:9200
3. Cháº¡y script nÃ y sáº½ tá»± Ä‘á»™ng táº£i dá»¯ liá»‡u vÃ o Elasticsearch

Náº¿u khÃ´ng cÃ³ Elasticsearch, chatbot sáº½ tá»± Ä‘á»™ng chuyá»ƒn sang sá»­ dá»¥ng DataFrame
"""

import pandas as pd
import re
from typing import List, Dict, Optional
from elasticsearch import Elasticsearch
import json
from flask import Flask, request, jsonify
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
from pyvi import ViTokenizer
import unicodedata

class RuleBasedChatbot:
    def __init__(self, data_file: str = 'data.xlsx', use_elasticsearch: bool = True):
        """Khá»Ÿi táº¡o chatbot vá»›i dá»¯ liá»‡u tá»« file Excel vÃ  Elasticsearch"""
        self.use_elasticsearch = use_elasticsearch
        self.index_name = "chatbot_data"
        
        if self.use_elasticsearch:
            try:
                # Káº¿t ná»‘i Ä‘áº¿n Elasticsearch (local instance)
                self.es = Elasticsearch([{'host': 'localhost', 'port': 9200}])
                
                # Kiá»ƒm tra káº¿t ná»‘i
                if not self.es.ping():
                    print("Cáº£nh bÃ¡o: KhÃ´ng thá»ƒ káº¿t ná»‘i Ä‘áº¿n Elasticsearch. Sá»­ dá»¥ng DataFrame thay tháº¿.")
                    self.use_elasticsearch = False
                    self._load_data_to_dataframe(data_file)
                else:
                    print("Káº¿t ná»‘i Elasticsearch thÃ nh cÃ´ng!")
                    self._setup_elasticsearch_index()
                    self._load_data_to_elasticsearch(data_file)
                    
            except Exception as e:
                print(f"Lá»—i khi káº¿t ná»‘i Elasticsearch: {e}")
                print("Sá»­ dá»¥ng DataFrame thay tháº¿.")
                self.use_elasticsearch = False
                self._load_data_to_dataframe(data_file)
        else:
            self._load_data_to_dataframe(data_file)
    
    def _load_data_to_dataframe(self, data_file: str):
        """Táº£i dá»¯ liá»‡u vÃ o DataFrame"""
        self.data = pd.read_excel(data_file)
        self.data = self.data.dropna(subset=['question', 'answer'])
        self.data['question'] = self.data['question'].astype(str).str.lower()
        self.data['answer'] = self.data['answer'].astype(str)
        
        # Preprocessing cho semantic search vá»›i nhiá»u variations
        self.data['processed_question'] = self.data['question'].apply(self.full_preprocess)
        self.data['simple_processed'] = self.data['question'].apply(self.preprocess_text)
        self.data['important_words'] = self.data['question'].apply(lambda x: ' '.join(self.extract_important_words(x)))
        
        # Káº¿t há»£p táº¥t cáº£ variations Ä‘á»ƒ táº¡o corpus phong phÃº hÆ¡n
        all_questions = []
        for idx, row in self.data.iterrows():
            combined = f"{row['processed_question']} {row['simple_processed']} {row['important_words']}"
            all_questions.append(combined)
        
        # Táº¡o TF-IDF vectorizer vá»›i tham sá»‘ tá»‘i Æ°u hÆ¡n
        self.tfidf_vectorizer = TfidfVectorizer(
            max_features=10000,  # TÄƒng sá»‘ features
            ngram_range=(1, 3),  # Bao gá»“m 1-gram, 2-gram, 3-gram
            min_df=1,           # Táº§n sá»‘ tá»‘i thiá»ƒu
            max_df=0.95,        # Táº§n sá»‘ tá»‘i Ä‘a
            lowercase=True,
            stop_words=None     # KhÃ´ng dÃ¹ng stop_words built-in
        )
        self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(all_questions)
        
        # Chá»‰ in thÃ´ng tin khi khÃ´ng cháº¡y API
        import sys
        if not (len(sys.argv) > 1 and sys.argv[1] == 'api'):
            print(f"ÄÃ£ táº£i {len(self.data)} cÃ¢u há»i tá»« {data_file}")
            print(f"ÄÃ£ táº¡o TF-IDF matrix vá»›i {self.tfidf_matrix.shape[1]} features")
    
    def _setup_elasticsearch_index(self):
        """Thiáº¿t láº­p index cho Elasticsearch"""
        index_mapping = {
            "mappings": {
                "properties": {
                    "question": {
                        "type": "text",
                        "analyzer": "standard"
                    },
                    "answer": {
                        "type": "text"
                    },
                    "nguon": {
                        "type": "text"
                    }
                }
            }
        }
        
        # XÃ³a index cÅ© náº¿u tá»“n táº¡i
        if self.es.indices.exists(index=self.index_name):
            self.es.indices.delete(index=self.index_name)
        
        # Táº¡o index má»›i
        self.es.indices.create(index=self.index_name, body=index_mapping)
    
    def _load_data_to_elasticsearch(self, data_file: str):
        """Táº£i dá»¯ liá»‡u tá»« Excel vÃ o Elasticsearch"""
        df = pd.read_excel(data_file)
        df = df.dropna(subset=['question', 'answer'])
        
        # ÄÆ°a dá»¯ liá»‡u vÃ o Elasticsearch
        for idx, row in df.iterrows():
            doc = {
                "question": str(row['question']).lower(),
                "answer": str(row['answer']),
                "nguon": str(row['nguá»“n']) if pd.notna(row['nguá»“n']) else ""
            }
            
            self.es.index(index=self.index_name, id=idx, body=doc)
        
        # Refresh index Ä‘á»ƒ Ä‘áº£m báº£o dá»¯ liá»‡u cÃ³ sáºµn
        self.es.indices.refresh(index=self.index_name)
        print(f"ÄÃ£ táº£i {len(df)} báº£n ghi vÃ o Elasticsearch!")
        
    def preprocess_text(self, text: str) -> str:
        """Xá»­ lÃ½ vÄƒn báº£n Ä‘áº§u vÃ o vá»›i Text Normalization vÃ  Noise Removal"""
        if not text:
            return ""
        
        # Text Normalization
        text = str(text).lower().strip()
        
        # Unicode normalization
        text = unicodedata.normalize('NFC', text)
        
        # Noise Removal - giá»¯ láº¡i nhiá»u kÃ½ tá»± hÆ¡n
        text = re.sub(r'[^\w\s]', ' ', text)  # Loáº¡i bá» kÃ½ tá»± Ä‘áº·c biá»‡t
        text = re.sub(r'\s+', ' ', text)      # Loáº¡i bá» khoáº£ng tráº¯ng thá»«a
        
        return text.strip()
    
    def tokenize_vietnamese(self, text: str) -> List[str]:
        """Tokenization cho tiáº¿ng Viá»‡t"""
        if not text:
            return []
        
        # Sá»­ dá»¥ng pyvi Ä‘á»ƒ tÃ¡ch tá»« tiáº¿ng Viá»‡t
        try:
            tokenized = ViTokenizer.tokenize(text)
            tokens = tokenized.split()
            return [token for token in tokens if len(token) > 0]  # Giáº£m tá»« 1 xuá»‘ng 0
        except:
            # Fallback náº¿u pyvi khÃ´ng hoáº¡t Ä‘á»™ng
            return text.split()
    
    def remove_stopwords(self, tokens: List[str]) -> List[str]:
        """Loáº¡i bá» stopwords tiáº¿ng Viá»‡t - Ã­t hÆ¡n Ä‘á»ƒ giá»¯ nhiá»u tá»« quan trá»ng"""
        stop_words = {
            'lÃ ', 'cá»§a', 'vÃ ', 'cÃ³', 'Ä‘Æ°á»£c', 'nÃ y', 'Ä‘Ã³', 'cho', 'vá»›i', 'vá»',
            'thÃ¬', 'mÃ ', 'rá»“i', 'Ä‘ang', 'váº«n', 'Ä‘á»u', 'cÅ©ng'
        }
        
        return [token for token in tokens if token not in stop_words and len(token) > 0]
    
    def extract_important_words(self, text: str) -> List[str]:
        """TrÃ­ch xuáº¥t tá»« quan trá»ng khÃ´ng loáº¡i bá» stopwords"""
        normalized = self.preprocess_text(text)
        tokens = self.tokenize_vietnamese(normalized)
        return [token for token in tokens if len(token) > 0]
    
    def full_preprocess(self, text: str) -> str:
        """Preprocessing Ä‘áº§y Ä‘á»§: Normalization + Tokenization + Stopword removal"""
        # Text Normalization vÃ  Noise Removal
        normalized = self.preprocess_text(text)
        
        # Tokenization
        tokens = self.tokenize_vietnamese(normalized)
        
        # Remove stopwords
        clean_tokens = self.remove_stopwords(tokens)
        
        return ' '.join(clean_tokens)
    
    def wildcard_match(self, user_input: str) -> Optional[str]:
        """TÃ¬m kiáº¿m wildcard pattern - linh hoáº¡t hÆ¡n"""
        input_words = self.extract_important_words(user_input)
        
        best_match = None
        max_matches = 0
        
        for idx, row in self.data.iterrows():
            question_words = self.extract_important_words(row['question'])
            
            # Äáº¿m sá»‘ tá»« khá»›p
            matches = 0
            for input_word in input_words:
                for question_word in question_words:
                    if len(input_word) > 1 and (input_word in question_word or question_word in input_word):
                        matches += 1
                        break
            
            # Cáº­p nháº­t best match
            if matches > max_matches and matches > 0:
                max_matches = matches
                best_match = row['answer']
        
        # Tráº£ vá» náº¿u cÃ³ Ã­t nháº¥t 1 tá»« khá»›p
        return best_match if max_matches > 0 else None
    
    def fuzzy_match(self, user_input: str) -> Optional[str]:
        """TÃ¬m kiáº¿m fuzzy matching - ngÆ°á»¡ng tháº¥p hÆ¡n"""
        input_words = set(self.extract_important_words(user_input))
        
        best_match = None
        best_similarity = 0
        
        for idx, row in self.data.iterrows():
            question_words = set(self.extract_important_words(row['question']))
            
            if input_words and question_words:
                # TÃ­nh similarity vá»›i nhiá»u cÃ¡ch khÃ¡c nhau
                intersection = len(input_words & question_words)
                union = len(input_words | question_words)
                
                # Jaccard similarity
                jaccard = intersection / union if union > 0 else 0
                
                # Containment similarity
                containment = intersection / len(input_words) if len(input_words) > 0 else 0
                
                # Láº¥y max cá»§a 2 similarity
                similarity = max(jaccard, containment)
                
                if similarity > best_similarity:
                    best_similarity = similarity
                    best_match = row['answer']
        
        # NgÆ°á»¡ng tháº¥p hÆ¡n Ä‘á»ƒ dá»… match hÆ¡n
        return best_match if best_similarity > 0.1 else None
    
    def match_phrase(self, user_input: str) -> Optional[str]:
        """TÃ¬m kiáº¿m match phrase - linh hoáº¡t hÆ¡n"""
        processed_input = self.preprocess_text(user_input)
        input_words = processed_input.split()
        
        best_match = None
        max_score = 0
        
        for idx, row in self.data.iterrows():
            question = self.preprocess_text(row['question'])
            question_words = question.split()
            
            score = 0
            
            # Kiá»ƒm tra subsequence matching
            for i in range(len(input_words)):
                for j in range(i + 1, len(input_words) + 1):
                    phrase = ' '.join(input_words[i:j])
                    if len(phrase) > 2 and phrase in question:
                        score += len(phrase)
            
            # Kiá»ƒm tra reverse matching
            for i in range(len(question_words)):
                for j in range(i + 1, len(question_words) + 1):
                    phrase = ' '.join(question_words[i:j])
                    if len(phrase) > 2 and phrase in processed_input:
                        score += len(phrase)
            
            if score > max_score:
                max_score = score
                best_match = row['answer']
        
        return best_match if max_score > 0 else None
    
    def semantic_search(self, user_input: str) -> Optional[str]:
        """TÃ¬m kiáº¿m semantic - ngÆ°á»¡ng tháº¥p hÆ¡n"""
        if not hasattr(self, 'tfidf_vectorizer') or not hasattr(self, 'tfidf_matrix'):
            return None
        
        # Preprocessing input vá»›i nhiá»u cÃ¡ch khÃ¡c nhau
        processed_options = [
            self.full_preprocess(user_input),
            self.preprocess_text(user_input),
            ' '.join(self.extract_important_words(user_input))
        ]
        
        best_match = None
        best_similarity = 0
        
        for processed_input in processed_options:
            if not processed_input:
                continue
            
            try:
                # Táº¡o vector cho input
                input_vector = self.tfidf_vectorizer.transform([processed_input])
                
                # TÃ­nh cosine similarity
                similarities = cosine_similarity(input_vector, self.tfidf_matrix)
                
                # Láº¥y similarity cao nháº¥t
                max_sim = np.max(similarities[0])
                
                if max_sim > best_similarity:
                    best_similarity = max_sim
                    best_match_idx = np.argmax(similarities[0])
                    best_match = self.data.iloc[best_match_idx]['answer']
            except:
                continue
        
        # NgÆ°á»¡ng ráº¥t tháº¥p Ä‘á»ƒ dá»… match
        return best_match if best_similarity > 0.01 else None
    
    def keyword_overlap_match(self, user_input: str) -> Optional[str]:
        """TÃ¬m kiáº¿m dá»±a trÃªn sá»‘ lÆ°á»£ng tá»« khÃ³a chung"""
        input_words = set(self.extract_important_words(user_input))
        
        best_match = None
        max_overlap = 0
        
        for idx, row in self.data.iterrows():
            question_words = set(self.extract_important_words(row['question']))
            
            # Äáº¿m sá»‘ tá»« chung
            overlap = len(input_words & question_words)
            
            if overlap > max_overlap:
                max_overlap = overlap
                best_match = row['answer']
        
        # Tráº£ vá» náº¿u cÃ³ Ã­t nháº¥t 1 tá»« chung
        return best_match if max_overlap > 0 else None
    
    def partial_string_match(self, user_input: str) -> Optional[str]:
        """TÃ¬m kiáº¿m chuá»—i con"""
        processed_input = self.preprocess_text(user_input)
        
        # TÃ¬m cÃ¢u há»i chá»©a input hoáº·c ngÆ°á»£c láº¡i
        for idx, row in self.data.iterrows():
            question = self.preprocess_text(row['question'])
            
            # Kiá»ƒm tra chuá»—i con vá»›i Ä‘á»™ dÃ i tá»‘i thiá»ƒu
            if len(processed_input) > 2 and processed_input in question:
                return row['answer']
            
            if len(question) > 2 and question in processed_input:
                return row['answer']
        
        return None
    
    def find_exact_match(self, user_input: str) -> Optional[str]:
        """TÃ¬m kiáº¿m khá»›p chÃ­nh xÃ¡c"""
        if self.use_elasticsearch:
            return self._find_exact_match_es(user_input)
        else:
            return self._find_exact_match_df(user_input)
    
    def _find_exact_match_df(self, user_input: str) -> Optional[str]:
        """TÃ¬m kiáº¿m khá»›p chÃ­nh xÃ¡c báº±ng DataFrame"""
        processed_input = self.preprocess_text(user_input)
        
        for idx, row in self.data.iterrows():
            if processed_input == self.preprocess_text(row['question']):
                return row['answer']
        return None
    
    def _find_exact_match_es(self, user_input: str) -> Optional[str]:
        """TÃ¬m kiáº¿m khá»›p chÃ­nh xÃ¡c báº±ng Elasticsearch"""
        processed_input = self.preprocess_text(user_input)
        
        query = {
            "query": {
                "match_phrase": {
                    "question": processed_input
                }
            }
        }
        
        try:
            response = self.es.search(index=self.index_name, body=query)
            if response['hits']['total']['value'] > 0:
                return response['hits']['hits'][0]['_source']['answer']
        except Exception as e:
            print(f"Lá»—i tÃ¬m kiáº¿m Elasticsearch: {e}")
        
        return None
    
    def get_response(self, user_input: str) -> str:
        """Láº¥y pháº£n há»“i cho cÃ¢u há»i cá»§a user vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p Ä‘Æ°á»£c cáº£i thiá»‡n"""
        if not user_input.strip():
            return "Xin chÃ o! TÃ´i cÃ³ thá»ƒ giÃºp gÃ¬ cho báº¡n?"
        
        # 1. Thá»­ exact match
        response = self.find_exact_match(user_input)
        if response:
            return response
        
        # 2. Thá»­ partial string match (dá»… match nháº¥t)
        response = self.partial_string_match(user_input)
        if response:
            return response
        
        # 3. Thá»­ keyword overlap match
        response = self.keyword_overlap_match(user_input)
        if response:
            return response
        
        # 4. Thá»­ wildcard matching
        response = self.wildcard_match(user_input)
        if response:
            return response
        
        # 5. Thá»­ fuzzy matching
        response = self.fuzzy_match(user_input)
        if response:
            return response
        
        # 6. Thá»­ match phrase
        response = self.match_phrase(user_input)
        if response:
            return response
        
        # 7. Thá»­ semantic search
        response = self.semantic_search(user_input)
        if response:
            return response
        
        return "Xin lá»—i, tÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin phÃ¹ há»£p vá»›i cÃ¢u há»i cá»§a báº¡n. Báº¡n cÃ³ thá»ƒ há»i cÃ¢u há»i khÃ¡c khÃ´ng?"
    
    def chat(self):
        """Báº¯t Ä‘áº§u cuá»™c trÃ² chuyá»‡n"""
        print("=== RULE-BASED CHATBOT ===")
        if self.use_elasticsearch:
            print("ðŸ” Sá»­ dá»¥ng Elasticsearch Ä‘á»ƒ tÃ¬m kiáº¿m")
        else:
            print("ðŸ“Š Sá»­ dá»¥ng DataFrame Ä‘á»ƒ tÃ¬m kiáº¿m")
        print("ChÃ o báº¡n! TÃ´i lÃ  chatbot há»— trá»£ tráº£ lá»i cÃ¢u há»i.")
        print("GÃµ 'quit', 'exit' hoáº·c 'bye' Ä‘á»ƒ thoÃ¡t.")
        print("-" * 50)
        
        while True:
            user_input = input("\nBáº¡n: ").strip()
            
            if user_input.lower() in ['quit', 'exit', 'bye', 'thoÃ¡t']:
                print("Bot: Táº¡m biá»‡t! ChÃºc báº¡n má»™t ngÃ y tá»‘t lÃ nh!")
                break
            
            response = self.get_response(user_input)
            print(f"Bot: {response}")

def main():
    """HÃ m chÃ­nh Ä‘á»ƒ cháº¡y chatbot"""
    try:
        chatbot = RuleBasedChatbot()
        chatbot.chat()
    except FileNotFoundError:
        print("Lá»—i: KhÃ´ng tÃ¬m tháº¥y file data.xlsx")
    except Exception as e:
        print(f"Lá»—i: {e}")

# Khá»Ÿi táº¡o Flask app
app = Flask(__name__)
chatbot = None

@app.route('/', methods=['POST'])
def chat_api():
    global chatbot
    try:
        # Khá»Ÿi táº¡o chatbot náº¿u chÆ°a cÃ³
        if chatbot is None:
            chatbot = RuleBasedChatbot(use_elasticsearch=False)
        
        data = request.get_json()
        user_input = data.get('chatInput', '')
        response = chatbot.get_response(user_input)
        return jsonify({'output': response})
    except Exception as e:
        return jsonify({'error': str(e)}), 500

if __name__ == "__main__":
    import sys
    if len(sys.argv) > 1 and sys.argv[1] == 'api':
        print("Khá»Ÿi Ä‘á»™ng API server táº¡i http://localhost:8000")
        app.run(host='0.0.0.0', port=8000, debug=True)
    else:
        main()
