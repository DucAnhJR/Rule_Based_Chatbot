"""
Rule-Based Chatbot vá»›i Elasticsearch

Äá»ƒ sá»­ dá»¥ng Elasticsearch:
1. Táº£i vÃ  cÃ i Ä‘áº·t Elasticsearch tá»«: https://www.elastic.co/downloads/elasticsearch
2. Cháº¡y Elasticsearch trÃªn localhost:9200
3. Cháº¡y script nÃ y sáº½ tá»± Ä‘á»™ng táº£i dá»¯ liá»‡u vÃ o Elasticsearch

Náº¿u khÃ´ng cÃ³ Elasticsearch, chatbot sáº½ tá»± Ä‘á»™ng chuyá»ƒn sang sá»­ dá»¥ng DataFrame
"""

import pandas as pd
import re
from typing import List, Dict, Optional
from elasticsearch import Elasticsearch
import json
from flask import Flask, request, jsonify
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
from pyvi import ViTokenizer
import unicodedata

class RuleBasedChatbot:
    def __init__(self, data_file: str = 'data.xlsx', use_elasticsearch: bool = True):
        """Khá»Ÿi táº¡o chatbot vá»›i dá»¯ liá»‡u tá»« file Excel vÃ  Elasticsearch"""
        self.use_elasticsearch = use_elasticsearch
        self.index_name = "chatbot_data"
        
        if self.use_elasticsearch:
            try:
                # Káº¿t ná»‘i Ä‘áº¿n Elasticsearch (local instance)
                self.es = Elasticsearch([{'host': 'localhost', 'port': 9200}])
                
                # Kiá»ƒm tra káº¿t ná»‘i
                if not self.es.ping():
                    print("Cáº£nh bÃ¡o: KhÃ´ng thá»ƒ káº¿t ná»‘i Ä‘áº¿n Elasticsearch. Sá»­ dá»¥ng DataFrame thay tháº¿.")
                    self.use_elasticsearch = False
                    self._load_data_to_dataframe(data_file)
                else:
                    print("Káº¿t ná»‘i Elasticsearch thÃ nh cÃ´ng!")
                    self._setup_elasticsearch_index()
                    self._load_data_to_elasticsearch(data_file)
                    
            except Exception as e:
                print(f"Lá»—i khi káº¿t ná»‘i Elasticsearch: {e}")
                print("Sá»­ dá»¥ng DataFrame thay tháº¿.")
                self.use_elasticsearch = False
                self._load_data_to_dataframe(data_file)
        else:
            self._load_data_to_dataframe(data_file)
    
    def _load_data_to_dataframe(self, data_file: str):
        """Táº£i dá»¯ liá»‡u vÃ o DataFrame"""
        self.data = pd.read_excel(data_file)
        self.data = self.data.dropna(subset=['question', 'answer'])
        self.data['question'] = self.data['question'].astype(str).str.lower()
        self.data['answer'] = self.data['answer'].astype(str)
        
        # Preprocessing cho semantic search vá»›i nhiá»u variations
        self.data['processed_question'] = self.data['question'].apply(self.full_preprocess)
        self.data['simple_processed'] = self.data['question'].apply(self.preprocess_text)
        self.data['important_words'] = self.data['question'].apply(lambda x: ' '.join(self.extract_important_words(x)))
        
        # Káº¿t há»£p táº¥t cáº£ variations Ä‘á»ƒ táº¡o corpus phong phÃº hÆ¡n
        all_questions = []
        for idx, row in self.data.iterrows():
            combined = f"{row['processed_question']} {row['simple_processed']} {row['important_words']}"
            all_questions.append(combined)
        
        # Táº¡o TF-IDF vectorizer vá»›i tham sá»‘ tá»‘i Æ°u hÆ¡n
        self.tfidf_vectorizer = TfidfVectorizer(
            max_features=10000,  # TÄƒng sá»‘ features
            ngram_range=(1, 3),  # Bao gá»“m 1-gram, 2-gram, 3-gram
            min_df=1,           # Táº§n sá»‘ tá»‘i thiá»ƒu
            max_df=0.95,        # Táº§n sá»‘ tá»‘i Ä‘a
            lowercase=True,
            stop_words=None     # KhÃ´ng dÃ¹ng stop_words built-in
        )
        self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(all_questions)
        
        # Chá»‰ in thÃ´ng tin khi khÃ´ng cháº¡y API
        import sys
        if not (len(sys.argv) > 1 and sys.argv[1] == 'api'):
            print(f"ÄÃ£ táº£i {len(self.data)} cÃ¢u há»i tá»« {data_file}")
            print(f"ÄÃ£ táº¡o TF-IDF matrix vá»›i {self.tfidf_matrix.shape[1]} features")
    
    def _setup_elasticsearch_index(self):
        """Thiáº¿t láº­p index cho Elasticsearch"""
        index_mapping = {
            "mappings": {
                "properties": {
                    "question": {
                        "type": "text",
                        "analyzer": "standard"
                    },
                    "answer": {
                        "type": "text"
                    },
                    "nguon": {
                        "type": "text"
                    }
                }
            }
        }
        
        # XÃ³a index cÅ© náº¿u tá»“n táº¡i
        if self.es.indices.exists(index=self.index_name):
            self.es.indices.delete(index=self.index_name)
        
        # Táº¡o index má»›i
        self.es.indices.create(index=self.index_name, body=index_mapping)
    
    def _load_data_to_elasticsearch(self, data_file: str):
        """Táº£i dá»¯ liá»‡u tá»« Excel vÃ o Elasticsearch"""
        df = pd.read_excel(data_file)
        df = df.dropna(subset=['question', 'answer'])
        
        # ÄÆ°a dá»¯ liá»‡u vÃ o Elasticsearch
        for idx, row in df.iterrows():
            doc = {
                "question": str(row['question']).lower(),
                "answer": str(row['answer']),
                "nguon": str(row['nguá»“n']) if pd.notna(row['nguá»“n']) else ""
            }
            
            self.es.index(index=self.index_name, id=idx, body=doc)
        
        # Refresh index Ä‘á»ƒ Ä‘áº£m báº£o dá»¯ liá»‡u cÃ³ sáºµn
        self.es.indices.refresh(index=self.index_name)
        print(f"ÄÃ£ táº£i {len(df)} báº£n ghi vÃ o Elasticsearch!")
        
    def preprocess_text(self, text: str) -> str:
        """Xá»­ lÃ½ vÄƒn báº£n Ä‘áº§u vÃ o vá»›i Text Normalization vÃ  Noise Removal"""
        if not text:
            return ""
        
        # Text Normalization
        text = str(text).lower().strip()
        
        # Unicode normalization
        text = unicodedata.normalize('NFC', text)
        
        # Noise Removal - giá»¯ láº¡i nhiá»u kÃ½ tá»± hÆ¡n
        text = re.sub(r'[^\w\s]', ' ', text)  # Loáº¡i bá» kÃ½ tá»± Ä‘áº·c biá»‡t
        text = re.sub(r'\s+', ' ', text)      # Loáº¡i bá» khoáº£ng tráº¯ng thá»«a
        
        return text.strip()
    
    def tokenize_vietnamese(self, text: str) -> List[str]:
        """Tokenization cho tiáº¿ng Viá»‡t"""
        if not text:
            return []
        
        # Sá»­ dá»¥ng pyvi Ä‘á»ƒ tÃ¡ch tá»« tiáº¿ng Viá»‡t
        try:
            tokenized = ViTokenizer.tokenize(text)
            tokens = tokenized.split()
            return [token for token in tokens if len(token) > 0]  # Giáº£m tá»« 1 xuá»‘ng 0
        except:
            # Fallback náº¿u pyvi khÃ´ng hoáº¡t Ä‘á»™ng
            return text.split()
    
    def remove_stopwords(self, tokens: List[str]) -> List[str]:
        """Loáº¡i bá» stopwords tiáº¿ng Viá»‡t - Ã­t hÆ¡n Ä‘á»ƒ giá»¯ nhiá»u tá»« quan trá»ng"""
        stop_words = {
            'lÃ ', 'cá»§a', 'vÃ ', 'cÃ³', 'Ä‘Æ°á»£c', 'nÃ y', 'Ä‘Ã³', 'cho', 'vá»›i', 'vá»',
            'thÃ¬', 'mÃ ', 'rá»“i', 'Ä‘ang', 'váº«n', 'Ä‘á»u', 'cÅ©ng'
        }
        
        return [token for token in tokens if token not in stop_words and len(token) > 0]
    
    def extract_important_words(self, text: str) -> List[str]:
        """TrÃ­ch xuáº¥t tá»« quan trá»ng khÃ´ng loáº¡i bá» stopwords"""
        normalized = self.preprocess_text(text)
        tokens = self.tokenize_vietnamese(normalized)
        return [token for token in tokens if len(token) > 0]
    
    def full_preprocess(self, text: str) -> str:
        """Preprocessing Ä‘áº§y Ä‘á»§: Normalization + Tokenization + Stopword removal"""
        # Text Normalization vÃ  Noise Removal
        normalized = self.preprocess_text(text)
        
        # Tokenization
        tokens = self.tokenize_vietnamese(normalized)
        
        # Remove stopwords
        clean_tokens = self.remove_stopwords(tokens)
        
        return ' '.join(clean_tokens)
    
    def wildcard_match(self, user_input: str) -> Optional[str]:
        """TÃ¬m kiáº¿m wildcard pattern - cáº£i thiá»‡n vá»›i weighted scoring"""
        input_words = self.extract_important_words(user_input)
        
        best_match = None
        best_score = 0
        
        for idx, row in self.data.iterrows():
            question_words = self.extract_important_words(row['question'])
            question_text = self.preprocess_text(row['question'])
            
            score = 0
            matched_words = 0
            
            for input_word in input_words:
                word_matched = False
                
                # Exact match trong tá»«
                if input_word in question_words:
                    score += 10
                    word_matched = True
                
                # Partial match trong tá»«
                for question_word in question_words:
                    if len(input_word) > 1:
                        if input_word in question_word:
                            score += 5
                            word_matched = True
                        elif len(question_word) > 1 and question_word in input_word:
                            score += 3
                            word_matched = True
                
                # Match trong toÃ n bá»™ cÃ¢u há»i
                if len(input_word) > 2 and input_word in question_text:
                    score += 2
                    word_matched = True
                
                if word_matched:
                    matched_words += 1
            
            # Bonus cho tá»· lá»‡ tá»« Ä‘Æ°á»£c match
            if len(input_words) > 0:
                match_ratio = matched_words / len(input_words)
                score += match_ratio * 10
            
            # Bonus cho Ä‘á»™ dÃ i tá»« khÃ³a
            for word in input_words:
                if len(word) > 3:
                    score += 1
            
            if score > best_score:
                best_score = score
                best_match = row['answer']
        
        return best_match if best_score > 3 else None
    
    def fuzzy_match(self, user_input: str) -> Optional[str]:
        """TÃ¬m kiáº¿m fuzzy matching - cáº£i thiá»‡n vá»›i weighted scoring"""
        input_words = set(self.extract_important_words(user_input))
        
        best_match = None
        best_similarity = 0
        
        for idx, row in self.data.iterrows():
            question_words = set(self.extract_important_words(row['question']))
            
            if input_words and question_words:
                # TÃ­nh similarity vá»›i nhiá»u cÃ¡ch khÃ¡c nhau
                intersection = len(input_words & question_words)
                union = len(input_words | question_words)
                
                # Jaccard similarity
                jaccard = intersection / union if union > 0 else 0
                
                # Containment similarity (cáº£ 2 chiá»u)
                containment1 = intersection / len(input_words) if len(input_words) > 0 else 0
                containment2 = intersection / len(question_words) if len(question_words) > 0 else 0
                
                # Weighted combination
                similarity = (jaccard * 0.4) + (containment1 * 0.4) + (containment2 * 0.2)
                
                # Bonus cho sá»‘ tá»« khá»›p nhiá»u
                if intersection > 1:
                    similarity += intersection * 0.1
                
                if similarity > best_similarity:
                    best_similarity = similarity
                    best_match = row['answer']
        
        # NgÆ°á»¡ng tháº¥p hÆ¡n Ä‘á»ƒ dá»… match hÆ¡n
        return best_match if best_similarity > 0.05 else None
    
    def match_phrase(self, user_input: str) -> Optional[str]:
        """TÃ¬m kiáº¿m match phrase - linh hoáº¡t hÆ¡n"""
        processed_input = self.preprocess_text(user_input)
        input_words = processed_input.split()
        
        best_match = None
        max_score = 0
        
        for idx, row in self.data.iterrows():
            question = self.preprocess_text(row['question'])
            question_words = question.split()
            
            score = 0
            
            # Kiá»ƒm tra subsequence matching
            for i in range(len(input_words)):
                for j in range(i + 1, len(input_words) + 1):
                    phrase = ' '.join(input_words[i:j])
                    if len(phrase) > 2 and phrase in question:
                        score += len(phrase)
            
            # Kiá»ƒm tra reverse matching
            for i in range(len(question_words)):
                for j in range(i + 1, len(question_words) + 1):
                    phrase = ' '.join(question_words[i:j])
                    if len(phrase) > 2 and phrase in processed_input:
                        score += len(phrase)
            
            if score > max_score:
                max_score = score
                best_match = row['answer']
        
        return best_match if max_score > 0 else None
    
    def semantic_search(self, user_input: str) -> Optional[str]:
        """TÃ¬m kiáº¿m semantic - ngÆ°á»¡ng tháº¥p hÆ¡n"""
        if not hasattr(self, 'tfidf_vectorizer') or not hasattr(self, 'tfidf_matrix'):
            return None
        
        # Preprocessing input vá»›i nhiá»u cÃ¡ch khÃ¡c nhau
        processed_options = [
            self.full_preprocess(user_input),
            self.preprocess_text(user_input),
            ' '.join(self.extract_important_words(user_input))
        ]
        
        best_match = None
        best_similarity = 0
        
        for processed_input in processed_options:
            if not processed_input:
                continue
            
            try:
                # Táº¡o vector cho input
                input_vector = self.tfidf_vectorizer.transform([processed_input])
                
                # TÃ­nh cosine similarity
                similarities = cosine_similarity(input_vector, self.tfidf_matrix)
                
                # Láº¥y similarity cao nháº¥t
                max_sim = np.max(similarities[0])
                
                if max_sim > best_similarity:
                    best_similarity = max_sim
                    best_match_idx = np.argmax(similarities[0])
                    best_match = self.data.iloc[best_match_idx]['answer']
            except:
                continue
        
        # NgÆ°á»¡ng ráº¥t tháº¥p Ä‘á»ƒ dá»… match
        return best_match if best_similarity > 0.01 else None
    
    def advanced_similarity_match(self, user_input: str) -> Optional[str]:
        """TÃ¬m kiáº¿m dá»±a trÃªn similarity nÃ¢ng cao - tá»‘i Æ°u tá»‘c Ä‘á»™"""
        input_words = self.extract_important_words(user_input)
        processed_input = self.preprocess_text(user_input)
        
        # Giá»›i háº¡n sá»‘ tá»« Ä‘á»ƒ tÄƒng tá»‘c Ä‘á»™
        if len(input_words) > 10:
            input_words = input_words[:10]
        
        best_match = None
        best_score = 0
        
        for idx, row in self.data.iterrows():
            question_words = self.extract_important_words(row['question'])
            processed_question = self.preprocess_text(row['question'])
            
            total_score = 0
            
            # 1. Exact word matching (nhanh)
            input_set = set(input_words)
            question_set = set(question_words)
            if input_set and question_set:
                exact_matches = len(input_set & question_set)
                total_score += exact_matches * 10
                
                # Early exit náº¿u cÃ³ nhiá»u exact matches
                if exact_matches >= len(input_words) * 0.7:
                    return row['answer']
            
            # 2. Partial word matching (giá»›i háº¡n)
            partial_count = 0
            for input_word in input_words[:5]:  # Chá»‰ kiá»ƒm tra 5 tá»« Ä‘áº§u
                for question_word in question_words[:10]:  # Chá»‰ kiá»ƒm tra 10 tá»« Ä‘áº§u
                    if len(input_word) > 2 and len(question_word) > 2:
                        if input_word in question_word or question_word in input_word:
                            total_score += 5
                            partial_count += 1
                            break
                if partial_count >= 3:  # Äá»§ rá»“i, khÃ´ng cáº§n kiá»ƒm tra thÃªm
                    break
            
            # 3. Sequence matching (Ä‘Æ¡n giáº£n hÃ³a)
            if len(processed_input) > 5 and processed_input in processed_question:
                total_score += 15
            
            if total_score > best_score:
                best_score = total_score
                best_match = row['answer']
        
        return best_match if best_score > 8 else None
    
    def levenshtein_match(self, user_input: str) -> Optional[str]:
        """TÃ¬m kiáº¿m dá»±a trÃªn khoáº£ng cÃ¡ch Levenshtein Ä‘Æ¡n giáº£n - tá»‘i Æ°u tá»‘c Ä‘á»™"""
        processed_input = self.preprocess_text(user_input)
        
        # Giá»›i háº¡n Ä‘á»™ dÃ i Ä‘á»ƒ tÄƒng tá»‘c Ä‘á»™
        if len(processed_input) > 50:
            processed_input = processed_input[:50]
        
        def simple_distance(s1, s2):
            # Tá»‘i Æ°u: kiá»ƒm tra Ä‘á»™ dÃ i trÆ°á»›c
            len_diff = abs(len(s1) - len(s2))
            if len_diff > min(len(s1), len(s2)) * 0.7:  # QuÃ¡ khÃ¡c biá»‡t
                return float('inf')
            
            if len(s1) == 0:
                return len(s2)
            if len(s2) == 0:
                return len(s1)
            
            # Ma tráº­n DP Ä‘Æ¡n giáº£n vá»›i giá»›i háº¡n
            if len(s1) > 30 or len(s2) > 30:  # QuÃ¡ dÃ i, bá» qua
                return float('inf')
            
            prev_row = list(range(len(s2) + 1))
            for i, c1 in enumerate(s1):
                curr_row = [i + 1]
                for j, c2 in enumerate(s2):
                    insertions = prev_row[j + 1] + 1
                    deletions = curr_row[j] + 1
                    substitutions = prev_row[j] + (c1 != c2)
                    curr_row.append(min(insertions, deletions, substitutions))
                prev_row = curr_row
            
            return prev_row[-1]
        
        best_match = None
        min_distance = float('inf')
        
        for idx, row in self.data.iterrows():
            processed_question = self.preprocess_text(row['question'])
            
            # Tá»‘i Æ°u: kiá»ƒm tra nhanh trÆ°á»›c
            if abs(len(processed_input) - len(processed_question)) > 20:
                continue
            
            if processed_input and processed_question:
                distance = simple_distance(processed_input[:30], processed_question[:30])  # Giá»›i háº¡n Ä‘á»™ dÃ i
                max_len = max(len(processed_input), len(processed_question))
                
                if max_len > 0 and distance != float('inf'):
                    similarity = 1 - (distance / max_len)
                    if similarity > 0.5 and distance < min_distance:  # TÄƒng ngÆ°á»¡ng
                        min_distance = distance
                        best_match = row['answer']
        
        return best_match
    
    def flexible_keyword_match(self, user_input: str) -> Optional[str]:
        """TÃ¬m kiáº¿m tá»« khÃ³a linh hoáº¡t vá»›i scoring"""
        input_words = [word.lower() for word in self.extract_important_words(user_input) if len(word) > 1]
        
        if not input_words:
            return None
        
        scored_matches = []
        
        for idx, row in self.data.iterrows():
            question_words = [word.lower() for word in self.extract_important_words(row['question']) if len(word) > 1]
            question_text = self.preprocess_text(row['question'])
            
            score = 0
            
            # Äiá»ƒm cho exact match
            for input_word in input_words:
                if input_word in question_words:
                    score += 10
                
                # Äiá»ƒm cho partial match
                for question_word in question_words:
                    if len(input_word) > 2 and input_word in question_word:
                        score += 5
                    elif len(question_word) > 2 and question_word in input_word:
                        score += 5
                
                # Äiá»ƒm cho substring trong cÃ¢u há»i
                if input_word in question_text:
                    score += 3
            
            # Bonus cho nhiá»u tá»« match
            matched_words = set(input_words) & set(question_words)
            if len(matched_words) > 1:
                score += len(matched_words) * 5
            
            if score > 0:
                scored_matches.append((score, row['answer']))
        
        if scored_matches:
            scored_matches.sort(reverse=True)
            return scored_matches[0][1]
        
        return None
    
    def ngram_match(self, user_input: str) -> Optional[str]:
        """TÃ¬m kiáº¿m dá»±a trÃªn n-gram"""
        def get_ngrams(text, n):
            words = text.split()
            return [' '.join(words[i:i+n]) for i in range(len(words) - n + 1)]
        
        processed_input = self.preprocess_text(user_input)
        input_bigrams = set(get_ngrams(processed_input, 2))
        input_trigrams = set(get_ngrams(processed_input, 3))
        
        best_match = None
        best_score = 0
        
        for idx, row in self.data.iterrows():
            processed_question = self.preprocess_text(row['question'])
            question_bigrams = set(get_ngrams(processed_question, 2))
            question_trigrams = set(get_ngrams(processed_question, 3))
            
            score = 0
            
            # Trigram matches (cao Ä‘iá»ƒm nháº¥t)
            trigram_matches = len(input_trigrams & question_trigrams)
            score += trigram_matches * 15
            
            # Bigram matches
            bigram_matches = len(input_bigrams & question_bigrams)
            score += bigram_matches * 8
            
            if score > best_score:
                best_score = score
                best_match = row['answer']
        
        return best_match if best_score > 10 else None
    
    def contextual_match(self, user_input: str) -> Optional[str]:
        """TÃ¬m kiáº¿m theo ngá»¯ cáº£nh"""
        # CÃ¡c tá»« chá»‰ thá»‹ quan trá»ng
        question_indicators = ['gÃ¬', 'sao', 'nÃ o', 'Ä‘Ã¢u', 'khi', 'nhÆ°', 'tháº¿', 'lÃ m', 'cÃ³', 'Ä‘Æ°á»£c']
        
        processed_input = self.preprocess_text(user_input)
        input_words = processed_input.split()
        
        best_match = None
        best_relevance = 0
        
        for idx, row in self.data.iterrows():
            processed_question = self.preprocess_text(row['question'])
            question_words = processed_question.split()
            
            relevance = 0
            
            # Kiá»ƒm tra tá»« chá»‰ thá»‹
            for indicator in question_indicators:
                if indicator in input_words and indicator in question_words:
                    relevance += 5
            
            # Kiá»ƒm tra tá»« khÃ³a trong ngá»¯ cáº£nh
            for i, word in enumerate(input_words):
                if word in question_words:
                    # Bonus cho vá»‹ trÃ­ tÆ°Æ¡ng tá»±
                    try:
                        question_index = question_words.index(word)
                        position_similarity = 1 - abs(i - question_index) / max(len(input_words), len(question_words))
                        relevance += position_similarity * 3
                    except:
                        relevance += 2
            
            if relevance > best_relevance:
                best_relevance = relevance
                best_match = row['answer']
        
        return best_match if best_relevance > 3 else None
    
    def partial_string_match(self, user_input: str) -> Optional[str]:
        """TÃ¬m kiáº¿m chuá»—i con"""
        processed_input = self.preprocess_text(user_input)
        
        # TÃ¬m cÃ¢u há»i chá»©a input hoáº·c ngÆ°á»£c láº¡i
        for idx, row in self.data.iterrows():
            question = self.preprocess_text(row['question'])
            
            # Kiá»ƒm tra chuá»—i con vá»›i Ä‘á»™ dÃ i tá»‘i thiá»ƒu
            if len(processed_input) > 2 and processed_input in question:
                return row['answer']
            
            if len(question) > 2 and question in processed_input:
                return row['answer']
        
        return None
    
    def find_exact_match(self, user_input: str) -> Optional[str]:
        """TÃ¬m kiáº¿m khá»›p chÃ­nh xÃ¡c"""
        if self.use_elasticsearch:
            return self._find_exact_match_es(user_input)
        else:
            return self._find_exact_match_df(user_input)
    
    def _find_exact_match_df(self, user_input: str) -> Optional[str]:
        """TÃ¬m kiáº¿m khá»›p chÃ­nh xÃ¡c báº±ng DataFrame"""
        processed_input = self.preprocess_text(user_input)
        
        for idx, row in self.data.iterrows():
            if processed_input == self.preprocess_text(row['question']):
                return row['answer']
        return None
    
    def _find_exact_match_es(self, user_input: str) -> Optional[str]:
        """TÃ¬m kiáº¿m khá»›p chÃ­nh xÃ¡c báº±ng Elasticsearch"""
        processed_input = self.preprocess_text(user_input)
        
        query = {
            "query": {
                "match_phrase": {
                    "question": processed_input
                }
            }
        }
        
        try:
            response = self.es.search(index=self.index_name, body=query)
            if response['hits']['total']['value'] > 0:
                return response['hits']['hits'][0]['_source']['answer']
        except Exception as e:
            print(f"Lá»—i tÃ¬m kiáº¿m Elasticsearch: {e}")
        
        return None
    
    def get_response(self, user_input: str) -> str:
        """Láº¥y pháº£n há»“i cho cÃ¢u há»i cá»§a user vá»›i há»‡ thá»‘ng tá»‘i Æ°u tá»‘c Ä‘á»™"""
        if not user_input.strip():
            return "Xin chÃ o! TÃ´i cÃ³ thá»ƒ giÃºp gÃ¬ cho báº¡n?"
        
        # Thá»© tá»± tá»‘i Æ°u: tá»« nhanh nháº¥t Ä‘áº¿n cháº­m nháº¥t
        
        # 1. Exact match (nhanh nháº¥t)
        exact_result = self.find_exact_match(user_input)
        if exact_result:
            return exact_result
        
        # 2. Partial string match (ráº¥t nhanh)
        partial_result = self.partial_string_match(user_input)
        if partial_result:
            return partial_result
        
        # 3. Flexible keyword match (nhanh, hiá»‡u quáº£ cao)
        flex_keyword_result = self.flexible_keyword_match(user_input)
        if flex_keyword_result:
            return flex_keyword_result
        
        # 4. Wildcard matching (nhanh)
        wildcard_result = self.wildcard_match(user_input)
        if wildcard_result:
            return wildcard_result
        
        # 5. N-gram match (trung bÃ¬nh)
        ngram_result = self.ngram_match(user_input)
        if ngram_result:
            return ngram_result
        
        # 6. Fuzzy matching (trung bÃ¬nh)
        fuzzy_result = self.fuzzy_match(user_input)
        if fuzzy_result:
            return fuzzy_result
        
        # 7. Contextual match (trung bÃ¬nh)
        context_result = self.contextual_match(user_input)
        if context_result:
            return context_result
        
        # 8. Semantic search (nhanh vá»›i TF-IDF)
        semantic_result = self.semantic_search(user_input)
        if semantic_result:
            return semantic_result
        
        # 9. Advanced similarity (cháº­m hÆ¡n - chá»‰ khi cáº§n thiáº¿t)
        adv_sim_result = self.advanced_similarity_match(user_input)
        if adv_sim_result:
            return adv_sim_result
        
        # 10. Levenshtein (cháº­m nháº¥t - cuá»‘i cÃ¹ng)
        leven_result = self.levenshtein_match(user_input)
        if leven_result:
            return leven_result
        
        return "Xin lá»—i, tÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin phÃ¹ há»£p vá»›i cÃ¢u há»i cá»§a báº¡n. Báº¡n cÃ³ thá»ƒ há»i cÃ¢u há»i khÃ¡c khÃ´ng?"
    
    def chat(self):
        """Báº¯t Ä‘áº§u cuá»™c trÃ² chuyá»‡n"""
        print("=== RULE-BASED CHATBOT ===")
        if self.use_elasticsearch:
            print("ðŸ” Sá»­ dá»¥ng Elasticsearch Ä‘á»ƒ tÃ¬m kiáº¿m")
        else:
            print("ðŸ“Š Sá»­ dá»¥ng DataFrame Ä‘á»ƒ tÃ¬m kiáº¿m")
        print("ChÃ o báº¡n! TÃ´i lÃ  chatbot há»— trá»£ tráº£ lá»i cÃ¢u há»i.")
        print("GÃµ 'quit', 'exit' hoáº·c 'bye' Ä‘á»ƒ thoÃ¡t.")
        print("-" * 50)
        
        while True:
            user_input = input("\nBáº¡n: ").strip()
            
            if user_input.lower() in ['quit', 'exit', 'bye', 'thoÃ¡t']:
                print("Bot: Táº¡m biá»‡t! ChÃºc báº¡n má»™t ngÃ y tá»‘t lÃ nh!")
                break
            
            response = self.get_response(user_input)
            print(f"Bot: {response}")

def main():
    """HÃ m chÃ­nh Ä‘á»ƒ cháº¡y chatbot"""
    try:
        chatbot = RuleBasedChatbot()
        chatbot.chat()
    except FileNotFoundError:
        print("Lá»—i: KhÃ´ng tÃ¬m tháº¥y file data.xlsx")
    except Exception as e:
        print(f"Lá»—i: {e}")

# Khá»Ÿi táº¡o Flask app
app = Flask(__name__)
chatbot = None

@app.route('/', methods=['POST'])
def chat_api():
    global chatbot
    try:
        # Khá»Ÿi táº¡o chatbot náº¿u chÆ°a cÃ³
        if chatbot is None:
            chatbot = RuleBasedChatbot(use_elasticsearch=False)
        
        data = request.get_json()
        user_input = data.get('chatInput', '')
        response = chatbot.get_response(user_input)
        return jsonify({'output': response})
    except Exception as e:
        return jsonify({'error': str(e)}), 500

if __name__ == "__main__":
    import sys
    if len(sys.argv) > 1 and sys.argv[1] == 'api':
        print("Khá»Ÿi Ä‘á»™ng API server táº¡i http://localhost:8000")
        app.run(host='0.0.0.0', port=8000, debug=True)
    else:
        main()
